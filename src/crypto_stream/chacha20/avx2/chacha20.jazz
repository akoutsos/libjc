u128 g_r16_u128 = (16u8)[13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2];
u256 g_r16      = (32u8)[13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2, 13,12,15,14, 9,8,11,10, 5,4,7,6, 1,0,3,2];
u256 g_r8       = (32u8)[14,13,12,15, 10,9,8,11, 6,5,4,7, 2,1,0,3, 14,13,12,15, 10,9,8,11, 6,5,4,7, 2,1,0,3];
u256 g_cnt      = (8u32)[7,6,5,4,3,2,1,0];
u256 g_cnt_inc  = (8u32)[8,8,8,8,8,8,8,8];
u256 g_p1       = (2u128)[1,0];
u256 g_p2       = (2u128)[2,2];

u128 g_sigma   = 0x6b20657479622d323320646e61707865;
u32 g_sigma0   = 0x61707865;
u32 g_sigma1   = 0x3320646e;
u32 g_sigma2   = 0x79622d32;
u32 g_sigma3   = 0x6b206574;

param int i_0 = 0;
param int i_1 = 1;
param int i_2 = 2;
param int i_3 = 3;
param int i_4 = 4;
param int i_5 = 5;
param int i_6 = 6;
param int i_7 = 7;
param int i_8 = 8;
param int i_9 = 9;
param int i_10 = 10;
param int i_11 = 11;
param int i_12 = 12;
param int i_13 = 13;
param int i_14 = 14;
param int i_15 = 15;
param int i_32 = 32;

fn init_x2(reg u64 key nonce, reg u32 counter) -> reg u256[4]
{
  reg u256[4] s;
  reg u256 k n;
  reg u128 t;
  stack u128 st;

  k = (u256)[key + 0];

  t = (u128)[nonce + 0];
  t = #x86_VPSLLDQ_128(t, 4);
  t = #x86_VPINSR_8u32(t, counter, 0);

  st = t;

  s[3] = #x86_VPBROADCAST_2u128(st);

  s[0] = #x86_VPBROADCAST_2u128(g_sigma);
  s[1] = #x86_VPERM2I128(k, k, 0x00);
  s[2] = #x86_VPERM2I128(k, k, 0x11);
  s[3] +8u32= g_p1;

  // s
  // 0 { sigma     , sigma      }
  // 1 { k[127:0]  , k[127:0]   }
  // 2 { k[255:128], k[255:128] }
  // 3 { n , cnt+1 , n , cnt    }
  return s;
}


fn init_x8(stack u256[16] st1_8, reg u64 k n1, reg u32 ctr) -> stack u256[16]
{
  inline int i;
  reg u256[16] s;
  stack u32 s_ctr;

  s_ctr = ctr;

  s[0] = #x86_VPBROADCAST_8u32(g_sigma0);
  s[1] = #x86_VPBROADCAST_8u32(g_sigma1);
  s[2] = #x86_VPBROADCAST_8u32(g_sigma2);
  s[3] = #x86_VPBROADCAST_8u32(g_sigma3);

  for i=0 to 8
  { s[i+4] = #x86_VPBROADCAST_8u32((u32)[k + i*4]); }

  s[12] = #x86_VPBROADCAST_8u32(s_ctr);
  s[12] +8u32= g_cnt;

  for i=0 to 3
  { s[i+13] = #x86_VPBROADCAST_8u32((u32)[n1 + i*4]); }

  st1_8 = s;

  //  k1_8
  //  0 { ... , sigma0     , sigma0     }
  //  1 { ... , sigma1     , sigma1     }
  //  2 { ... , sigma2     , sigma2     }
  //  3 { ... , sigma3     , sigma3     }
  //  4 { ... , k[31:0]    , k[31:0]    }
  // ...
  // 11 { ... , k[255:224] , k[255:224] }
  // 12 { ... , ctr+1      , ctr        }
  // 13 { ... , n[31:0]    , n[31:0]    }
  // ...
  // 15 { ... , n[95:64]   , n[95:64]   }
  return st1_8;
}


fn copy_state_x2(reg u256[4] st1) -> reg u256[4]
{
  reg u256[4] k1;
  k1 = st1;
  return k1;
}


fn copy_state_x4(reg u256[4] st1) -> reg u256[4], reg u256[4]
{
  reg u256[4] k1 k2;
  k1 = st1;
  k2 = st1;
  k2[3] +8u32= g_p2;

  // k2                         k1
  // { sigma     , sigma      } { sigma     , sigma      }
  // { k[127:0]  , k[127:0]   } { k[127:0]  , k[127:0]   }
  // { k[255:128], k[255:128] } { k[255:128], k[255:128] }
  // { n , cnt+3 , n , cnt+2  } { n , cnt+1 , n , cnt    }
  return k1, k2;
}


fn copy_state_x8(stack u256[16] st1_8) -> reg u256[16]
{
  reg u256[16] k1_8;
  k1_8 = st1_8;
  return k1_8;
}


fn sum_states_x2(reg u256[4] k12 st12) -> reg u256[4]
{
  inline int i;

  for i=0 to 4
  { k12[i] +8u32= st12[i]; }

  return k12;
}


fn sum_states_x4(reg u256[4] k12 k34 st12) -> reg u256[4], reg u256[4]
{
  inline int i;

  k12 = sum_states_x2(k12, st12);
  k34 = sum_states_x2(k34, st12);
  k34[3] +8u32= g_p2;

  return k12, k34;
}


fn sum_states_x8(reg u256[16] k, stack u256[16] s) -> reg u256[16]
{
  inline int i;

  for i=0 to 16
  { k[i] +8u32= s[i]; }

  return k;
}


fn perm_x2(reg u256[4] k) -> reg u256[4]
{
  reg u256[4] pk;

  pk[0] = #x86_VPERM2I128(k[0], k[1], 0x20);
  pk[1] = #x86_VPERM2I128(k[2], k[3], 0x20);
  pk[2] = #x86_VPERM2I128(k[0], k[1], 0x31);
  pk[3] = #x86_VPERM2I128(k[2], k[3], 0x31);

  return pk;
}


fn perm_x4(reg u256[4] k1 k2) -> reg u256[4], reg u256[4]
{
  reg u256[4] pk1 pk2;

  pk1 = perm_x2(k1);
  pk2 = perm_x2(k2);

  return pk1, pk2;
}


fn sub_rotate(reg u256[8] t) -> reg u256[8]
{
  inline int i;
  reg u256[8] x;

  x[0] = #x86_VPUNPCKL_4u64(t[0], t[1]);
  x[1] = #x86_VPUNPCKL_4u64(t[2], t[3]);
  x[2] = #x86_VPUNPCKH_4u64(t[0], t[1]);
  x[3] = #x86_VPUNPCKH_4u64(t[2], t[3]);

  x[4] = #x86_VPUNPCKL_4u64(t[4], t[5]);
  x[5] = #x86_VPUNPCKL_4u64(t[6], t[7]);
  x[6] = #x86_VPUNPCKH_4u64(t[4], t[5]);
  x[7] = #x86_VPUNPCKH_4u64(t[6], t[7]);

  for i=0 to 4
  {   t[i] = #x86_VPERM2I128(x[i*2+0], x[i*2+1], 0x20);
    t[i+4] = #x86_VPERM2I128(x[i*2+0], x[i*2+1], 0x31);
  }

  return t;
}

fn rotate(reg u256[8] x) -> reg u256[8]
{
  inline int i;
  reg u256[8] t;

  for i=0 to 4
  {   t[i] = #x86_VPUNPCKL_8u32(x[i*2+0], x[i*2+1]);
    t[i+4] = #x86_VPUNPCKH_8u32(x[i*2+0], x[i*2+1]);
  }

  t = sub_rotate(t);

  return t;
}

fn rotate_stack(stack u256[8] s) -> reg u256[8]
{
  inline int i;
  reg u256[8] t;
  reg u256[8] x;

  for i=0 to 4
  { x[i] = s[i*2+0]; }

  for i=0 to 4
  {   t[i] = #x86_VPUNPCKL_8u32(x[i], s[i*2+1]);
    t[i+4] = #x86_VPUNPCKH_8u32(x[i], s[i*2+1]);
  }

  t = sub_rotate(t);

  return t;
}

fn rotate_first_half_x8(reg u256[16] k1_8) -> reg u256[8], stack u256[8]
{
  inline int i;
  stack u256[8] s5_8;
  reg   u256[8] k1_4;

  for i=0 to 8
  { s5_8[i] = k1_8[i+8]; }

  for i=0 to 8
  { k1_4[i] = k1_8[i]; }

  k1_4 = rotate(k1_4);

  return k1_4, s5_8;
}

fn rotate_second_half_x8(stack u256[8] s5_8) -> reg u256[8]
{
  inline int i;
  reg u256[8] k5_8;

  k5_8 = rotate_stack(s5_8);

  return k5_8;
}

fn update_ptr(reg u64 output plain, reg u32 len, inline int n) -> reg u64, reg u64, reg u32
{
  output += n;
  plain += n;
  len -= n;
  return output, plain, len;
}

fn store(reg u64 output plain, reg u32 len, reg u256 k1 k2) -> reg u64, reg u64, reg u32
{
  k1 ^= (u256)[plain + 0*32];
  k2 ^= (u256)[plain + 1*32];

  (u256)[output + 0*32] = k1;
  (u256)[output + 1*32] = k2;

  output, plain, len = update_ptr(output, plain, len, 64);

  return output, plain, len;
}

fn store_last(reg u64 output plain, reg u32 len, reg u256 k1 k2)
{
  reg u256 r0;
  reg u128 r1;
  reg u64  r2;
  reg u8   r3;

  r0 = k1;

  if(len >= 32)
  {
    r0 ^= (u256)[plain + 0];
    (u256)[output + 0] = r0;

    output, plain, len = update_ptr(output, plain, len, 32);

    r0 = k2;
  }

  r1 = #x86_VEXTRACTI128(r0, 0);

  if(len >= 16)
  {
    r1 ^= (u128)[plain + 0];
    (u128)[output + 0] = r1;

    output, plain, len = update_ptr(output, plain, len, 16);

    r1 = #x86_VEXTRACTI128(r0, 1);
  }

  r2 = #x86_VPEXTR_64(r1, 0);

  if(len >= 8)
  {
    r2 ^= (u64)[plain + 0];
    (u64)[output + 0] = r2;

    output, plain, len = update_ptr(output, plain, len, 8);

    r2 = #x86_VPEXTR_64(r1, 1);
  }

  while(len > 0)
  {
    r3 = r2;
    r3 ^= (u8)[plain + 0];
    (u8)[output + 0] = r3;
    r2 >>= 8;

    output, plain, len = update_ptr(output, plain, len, 1);
  }
}

fn store_x2(reg u64 output plain, reg u32 len, reg u256[4] k) -> reg u64, reg u64, reg u32, reg u256[4]
{
  inline int i;

  for i=0 to 4
  { k[i] ^= (u256)[plain + i*32]; }

  for i=0 to 4
  { (u256)[output + i*32] = k[i]; }

  output, plain, len = update_ptr(output, plain, len, 128);

  return output, plain, len, k;
}

fn store_x2_last(reg u64 output plain, reg u32 len, reg u256[4] k)
{
  reg u256 r0, r1;

  r0 = k[0];
  r1 = k[1];

  if(len >= 64)
  {
    output, plain, len = store(output, plain, len, r0, r1);
    r0 = k[2];
    r1 = k[3];
  }

  store_last(output, plain, len, r0, r1);
}

fn store_x4(reg u64 output plain, reg u32 len, reg u256[8] k) -> reg u64, reg u64, reg u32
{
  inline int i;

  for i=0 to 8
  { k[i] ^= (u256)[plain + i*32]; }

  for i=0 to 8
  { (u256)[output + i*32] = k[i]; }

  output, plain, len = update_ptr(output, plain, len, 256);

  return output, plain, len;
}

fn store_x4_last(reg u64 output plain, reg u32 len, reg u256[8] k)
{
  inline int i;
  reg u256[4] r, r1;

  for i=0 to 4 { r[i] = k[i]; }

  if(len >= 128)
  {
    output, plain, len, r = store_x2(output, plain, len, r);
    for i=0 to 4 { r[i] = k[i+4]; }
  }

  store_x2_last(output, plain, len, r);
}

fn store_half_x8(reg u64 output plain, reg u32 len, reg u256[8] k1_4, inline int o)
{
  inline int i;

  for i=0 to 8
  { k1_4[i] ^= (u256)[plain + o + i*64]; }
  for i=0 to 8
  { (u256)[output + o + i*64] = k1_4[i]; }
}

fn store_x8(reg u64 output plain, reg u32 len, reg u256[16] k1_8) -> reg u64, reg u64, reg u32
{
  stack u256[8] s5_8;
  reg   u256[8] k1_4, k5_8;

  k1_4, s5_8 = rotate_first_half_x8(k1_8);
  store_half_x8(output, plain, len, k1_4, i_0);
  k5_8 = rotate_second_half_x8(s5_8);
  store_half_x8(output, plain, len, k5_8, i_32);

  output, plain, len = update_ptr(output, plain, len, 512);

  return output, plain, len;
}

fn interleave(stack u256[8] s1_4, reg u256[8] k5_8, inline int o) -> reg u256[8]
{
  inline int i;
  reg u256[8] w1_4;

  for i=0 to 4
  { w1_4[i*2+0] = s1_4[i+o];
    w1_4[i*2+1] = k5_8[i+o];
  }

  return w1_4;
}

fn store_x8_last(reg u64 output plain, reg u32 len, reg u256[16] k1_8)
{
  inline int i;
  stack u256[8] s1_4 s5_8;
  reg   u256[8] k1_4 k5_8 w1_4;

  k1_4, s5_8 = rotate_first_half_x8(k1_8);
  s1_4 = k1_4;
  k5_8 = rotate_second_half_x8(s5_8);
  w1_4 = interleave(s1_4, k5_8, i_0);

  if(len >= 256)
  {
    output, plain, len = store_x4(output, plain, len, w1_4);
    w1_4 = interleave(s1_4, k5_8, i_4);
  }

  store_x4_last(output, plain, len, w1_4);
}

fn increment_counter_x8(stack u256[16] s) -> stack u256[16]
{
  reg u256 t;

  t = g_cnt_inc;
  t +8u32= s[12];
  s[12] = t;

  return s;
}

fn round_128(reg u256[4] v0) -> reg u256[4]
{
  reg u256 t;

  v0[0] +8u32= v0[1];
  v0[3]     ^= v0[0];
  v0[3] = #x86_VPSHUFB_256(v0[3], g_r16);

  v0[2] +8u32= v0[3];
  v0[1]     ^= v0[2];

      t = v0[1] <<8u32 12;
  v0[1] = v0[1] >>8u32 20;
  v0[1] ^= t;

  v0[0] +8u32= v0[1];
  v0[3]     ^= v0[0];
  v0[3] = #x86_VPSHUFB_256(v0[3], g_r8);

  v0[2] +8u32= v0[3];
  v0[1] ^= v0[2];

      t = v0[1] <<8u32 7;
  v0[1] = v0[1] >>8u32 25;
  v0[1] ^= t;

  return v0;
}

fn round_256(
  reg u256[4] v0,
  reg u256[4] v4
) -> reg u256[4], reg u256[4]
{
  reg u256 t;

  v0[0] +8u32= v0[1];
  v4[0] +8u32= v4[1];

  v0[3] ^= v0[0];
  v4[3] ^= v4[0];

  v0[3] = #x86_VPSHUFB_256(v0[3], g_r16);
  v4[3] = #x86_VPSHUFB_256(v4[3], g_r16);

  v0[2] +8u32= v0[3];
  v4[2] +8u32= v4[3];

  v0[1] ^= v0[2];
  v4[1] ^= v4[2];

      t = v0[1] <<8u32 12;
  v0[1] = v0[1] >>8u32 20;
  v0[1] ^= t;

      t = v4[1] <<8u32 12;
  v4[1] = v4[1] >>8u32 20;
  v4[1] ^= t;

  v0[0] +8u32= v0[1];
  v4[0] +8u32= v4[1];

  v0[3] ^= v0[0];
  v4[3] ^= v4[0];

  v0[3] = #x86_VPSHUFB_256(v0[3], g_r8);
  v4[3] = #x86_VPSHUFB_256(v4[3], g_r8);

  v0[2] +8u32= v0[3];
  v4[2] +8u32= v4[3];

  v0[1] ^= v0[2];
  v4[1] ^= v4[2];

      t = v0[1] <<8u32 7;
  v0[1] = v0[1] >>8u32 25;
  v0[1] ^= t;

      t = v4[1] <<8u32 7;
  v4[1] = v4[1] >>8u32 25;
  v4[1] ^= t;

  return v0, v4;
}

fn shuffle_state(reg u256[4] v) -> reg u256[4]
{
  v[1] = #x86_VPSHUFD_256(v[1], (4u2)[0,3,2,1]);
  v[2] = #x86_VPSHUFD_256(v[2], (4u2)[1,0,3,2]);
  v[3] = #x86_VPSHUFD_256(v[3], (4u2)[2,1,0,3]);
  return v;
}

fn reverse_shuffle_state(reg u256[4] v) -> reg u256[4]
{
  v[1] = #x86_VPSHUFD_256(v[1], (4u2)[2,1,0,3]);
  v[2] = #x86_VPSHUFD_256(v[2], (4u2)[1,0,3,2]);
  v[3] = #x86_VPSHUFD_256(v[3], (4u2)[0,3,2,1]);
  return v;
}

fn rounds_x2(
  reg u256[4] v0
) -> reg u256[4]
{
  reg u64 c;

  c = 0;
  while(c < 10)
  {
    v0 = round_128(v0);
    v0 = shuffle_state(v0);
    v0 = round_128(v0);
    v0 = reverse_shuffle_state(v0);
    c += 1;
  }
  return v0;
}

fn rounds_x4(
  reg u256[4] v0,
  reg u256[4] v4
) -> reg u256[4], reg u256[4]
{
  reg u64 c;

  c = 0;
  while(c < 10)
  {
    v0, v4 = round_256(v0, v4);
        v0 = shuffle_state(v0);
        v4 = shuffle_state(v4);
    v0, v4 = round_256(v0, v4);
        v0 = reverse_shuffle_state(v0);
        v4 = reverse_shuffle_state(v4);
    c += 1;
  }
  return v0, v4;
}

fn double_quarter_round_512(
  reg u256[16] x,
  inline int a0, inline int b0, inline int c0, inline int d0,
  inline int a1, inline int b1, inline int c1, inline int d1
) -> reg u256[16]
{
  reg u256 t, r16;

    r16 = #x86_VPBROADCAST_2u128(g_r16_u128);

  x[a0] +8u32= x[b0];
  x[a1] +8u32= x[b1];

  x[d0] ^= x[a0];
  x[d1] ^= x[a1];

  x[d0] = #x86_VPSHUFB_256(x[d0], r16);
  x[d1] = #x86_VPSHUFB_256(x[d1], r16);

  x[c0] +8u32= x[d0];
  x[c1] +8u32= x[d1];

  x[b0] ^= x[c0];
  x[b1] ^= x[c1];

      t = x[b0] <<8u32 12;
  x[b0] = x[b0] >>8u32 20;
  x[b0] ^= t;

      t = x[b1] <<8u32 12;
  x[b1] = x[b1] >>8u32 20;
  x[b1] ^= t;

  x[a0] +8u32= x[b0];
  x[a1] +8u32= x[b1];

  x[d0] ^= x[a0];
  x[d1] ^= x[a1];

  x[d0] = #x86_VPSHUFB_256(x[d0], g_r8);
  x[d1] = #x86_VPSHUFB_256(x[d1], g_r8);

  x[c0] +8u32= x[d0];
  x[c1] +8u32= x[d1];

                          x[b0] ^= x[c0];
      t = x[b0] <<8u32 7;
  x[b0] = x[b0] >>8u32 25;
                          x[b1] ^= x[c1];
  x[b0] ^= t;

      t = x[b1] <<8u32 7;
  x[b1] = x[b1] >>8u32 25;
  x[b1] ^= t;

  return x;
}

fn doublex2_quarter_round_512(
  reg u256[16] x,
  stack u256 xd3_1,
  inline int a0, inline int b0, inline int c0, inline int d0,
  inline int a1, inline int b1, inline int c1, inline int d1,
  inline int a2, inline int b2, inline int c2, inline int d2,
  inline int a3, inline int b3, inline int c3, inline int d3
) -> reg u256[16], stack u256
{
  x = double_quarter_round_512(x, a0, b0, c0, d0, a1, b1, c1, d1);
  x[d3] = xd3_1;
  xd3_1 = x[d1];
  x = double_quarter_round_512(x, a2, b2, c2, d2, a3, b3, c3, d3);
  return x, xd3_1;
}

fn rounds_x8(reg u256[16] x) -> reg u256[16]
{
  reg u64 c;
  stack u256 x0, x14, x15;

  x15 = x[15];

  c = 0;
  while(c < 10)
  {
    x, x14 = doublex2_quarter_round_512(x, x15, i_0, i_4, i_8,  i_12,
                                                i_2, i_6, i_10, i_14,
                                                i_1, i_5, i_9,  i_13,
                                                i_3, i_7, i_11, i_15);

    x, x15 = doublex2_quarter_round_512(x, x14, i_1, i_6, i_11, i_12,
                                                i_0, i_5, i_10, i_15,
                                                i_2, i_7, i_8,  i_13,
                                                i_3, i_4, i_9,  i_14);
    c += 1;
  }

  x[15] = x15;

  return x;
}

fn chacha20_more_than_256(reg u64 output plain, reg u32 len, reg u64 k n1, reg u32 ctr)
{
  stack u256[16] st1_8;
  reg   u256[16]  k1_8;

  st1_8 = init_x8(st1_8, k, n1, ctr);

  while(len >= 512)
  {
    k1_8 = copy_state_x8(st1_8);
    k1_8 = rounds_x8(k1_8);
    k1_8 = sum_states_x8(k1_8, st1_8);
    output, plain, len = store_x8(output, plain, len, k1_8);
    st1_8 = increment_counter_x8(st1_8);
  }

  if(len > 0)
  {
    k1_8 = copy_state_x8(st1_8);
    k1_8 = rounds_x8(k1_8);
    k1_8 = sum_states_x8(k1_8, st1_8);
    store_x8_last(output, plain, len, k1_8);
  }

}

fn chacha20_less_than_257(reg u64 output plain, reg u32 len, reg u64 k n1, reg u32 ctr)
{
  reg u256[4] st12 k12 k34;

  st12 = init_x2(k, n1, ctr); 

  if(len > 128)
  {
    k12, k34 = copy_state_x4(st12);
    k12, k34 = rounds_x4(k12, k34);
    k12, k34 = sum_states_x4(k12, k34, st12);
    k12, k34 = perm_x4(k12, k34);
    output, plain, len, k12 = store_x2(output, plain, len, k12);
                              store_x2_last(output, plain, len, k34);
  }
  else
  {
    k12 = copy_state_x2(st12);
    k12 = rounds_x2(k12);
    k12 = sum_states_x2(k12, st12);
    k12 = perm_x2(k12);
          store_x2_last(output, plain, len, k12);
  }
}

export fn chacha20_avx2(reg u64 output plain, reg u32 len, reg u64 k n1, reg u32 ctr)
{
  if(len < 257)
  { chacha20_less_than_257(output, plain, len, k, n1, ctr); }
  else
  { chacha20_more_than_256(output, plain, len, k, n1, ctr); }
}

